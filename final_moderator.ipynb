{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspellchecker in c:\\users\\kiit\\anaconda3\\lib\\site-packages (0.5.4)\n",
      "Requirement already satisfied: pylanguagetool in c:\\users\\kiit\\anaconda3\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: configargparse>=0.14.0 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from pylanguagetool) (1.2)\n",
      "Requirement already satisfied: colorama>=0.4.1 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from pylanguagetool) (0.4.1)\n",
      "Requirement already satisfied: requests>=2.22.0 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from pylanguagetool) (2.22.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from requests>=2.22.0->pylanguagetool) (2019.11.28)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from requests>=2.22.0->pylanguagetool) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from requests>=2.22.0->pylanguagetool) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from requests>=2.22.0->pylanguagetool) (1.24.2)\n",
      "Requirement already satisfied: pymysql in c:\\users\\kiit\\anaconda3\\lib\\site-packages (0.9.3)\n",
      "Requirement already satisfied: gensim in c:\\users\\kiit\\anaconda3\\lib\\site-packages (3.8.2)\n",
      "Requirement already satisfied: scipy>=1.0.0 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from gensim) (1.2.1)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from gensim) (1.12.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from gensim) (1.11.1)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from gensim) (1.18.1)\n",
      "Requirement already satisfied: boto in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from smart-open>=1.8.1->gensim) (2.49.0)\n",
      "Requirement already satisfied: requests in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from smart-open>=1.8.1->gensim) (2.22.0)\n",
      "Requirement already satisfied: boto3 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.12.39)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (1.24.2)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2019.11.28)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.39 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from boto3->smart-open>=1.8.1->gensim) (1.15.39)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from boto3->smart-open>=1.8.1->gensim) (0.9.5)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from boto3->smart-open>=1.8.1->gensim) (0.3.3)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from botocore<1.16.0,>=1.15.39->boto3->smart-open>=1.8.1->gensim) (0.14)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from botocore<1.16.0,>=1.15.39->boto3->smart-open>=1.8.1->gensim) (2.7.5)\n",
      "Requirement already satisfied: textblob in c:\\users\\kiit\\anaconda3\\lib\\site-packages (0.15.3)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from textblob) (3.4.4)\n",
      "Requirement already satisfied: six in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (1.12.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\kiit\\anaconda3\\lib\\site-packages (3.4.4)\n",
      "Requirement already satisfied: six in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from nltk) (1.12.0)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'clean course description'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2656\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2657\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2658\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'clean course description'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-d3b6b327a479>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[1;31m#calling spelling correction module\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 151\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"clean course description\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSpelling_correction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2925\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2926\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2927\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2928\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2929\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2657\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2658\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2659\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2660\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2661\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'clean course description'"
     ]
    }
   ],
   "source": [
    "#Install all the dependencies and libraries required\n",
    "\n",
    "!pip install pyspellchecker\n",
    "!pip install pylanguagetool\n",
    "!pip install pymysql\n",
    "!pip install gensim\n",
    "!pip install textblob\n",
    "!pip install nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "import pymysql\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from gensim.parsing.preprocessing import remove_stopwords,stem_text,strip_tags\n",
    "from spellchecker import SpellChecker\n",
    "import re\n",
    "from textblob import TextBlob \n",
    "\n",
    "#################################################################################################################\n",
    "\n",
    "def Spelling_correction(string):\n",
    "    \n",
    "    print(\"original text: \"+str(string)) \n",
    "    b = TextBlob(string) \n",
    "    return(str(b.correct()))\n",
    "TextBlob(j).correct()\n",
    "##################################################################################################################\n",
    "\n",
    "def grammer_check(string):\n",
    "    \n",
    "\n",
    "    from pylanguagetool import api\n",
    "   \n",
    "    if(api.check(string,api_url='https://languagetool.org/api/v2/',lang='en-US', )['matches']==[]):\n",
    "        return(\"no gramatical error.\")\n",
    "    else:\n",
    "        s=api.check(string,api_url='https://languagetool.org/api/v2/',lang='en-US', )['matches'][0]['message']\n",
    "        return(s)\n",
    "\n",
    "##################################################################################################################\n",
    "\n",
    "def validation(df1):    \n",
    "    \n",
    "    #function for finding similarity\n",
    "    def similarity(X,Y):    \n",
    "        # Program to measure similarity between  \n",
    "        # two sentences using cosine similarity. \n",
    "        X=str(X)\n",
    "        Y=str(Y)\n",
    "\n",
    "        X = X.lower() \n",
    "        Y = Y.lower() \n",
    "\n",
    "        # tokenization \n",
    "        X_list = word_tokenize(X)  \n",
    "        Y_list = word_tokenize(Y) \n",
    "\n",
    "        # sw contains the list of stopwords \n",
    "        sw = stopwords.words('english')  \n",
    "        l1 =[];l2 =[] \n",
    "\n",
    "        # remove stop words from string \n",
    "        X_set = {w for w in X_list if not w in sw}  \n",
    "        Y_set = {w for w in Y_list if not w in sw} \n",
    "\n",
    "        # form a set containing keywords of both strings  \n",
    "        rvector = X_set.union(Y_set)  \n",
    "        for w in rvector: \n",
    "            if w in X_set: l1.append(1) # create a vector \n",
    "            else: l1.append(0) \n",
    "            if w in Y_set: l2.append(1) \n",
    "            else: l2.append(0) \n",
    "        c = 0\n",
    "\n",
    "        # cosine formula  \n",
    "        for i in range(len(rvector)): \n",
    "                c+= l1[i]*l2[i] \n",
    "\n",
    "        if(float((sum(l1)*sum(l2))**0.5) ==0.0):\n",
    "            return(0)\n",
    "        else:\n",
    "            cosine = c / float((sum(l1)*sum(l2))**0.5)\n",
    "            return cosine\n",
    "#################################################################################################################\n",
    "    \n",
    "    #download the edx dataset\n",
    "    edx=pd.read_csv(\"EDX_data.csv\")\n",
    "\n",
    "################################################################################################################\n",
    "\n",
    "    #Create a dataframe matching the titles of the courses for verification of description\n",
    "    l_f=[]\n",
    "    l=[]\n",
    "    df1_count=0\n",
    "    for i in df1[\"course title\"]:\n",
    "        edx_count=0\n",
    "        for j in edx[\"Name\"]:\n",
    "            if((((similarity(i,j)>0.45) and (similarity(df1[\"clean course description\"][df1_count],str(edx[\"Description\"][edx_count]))>=0.0))\n",
    "            or((similarity(i,j)>0.45) and (similarity(df1[\"clean course description\"][df1_count],str(edx[\"Short Description\"][edx_count]))>=0.0))\n",
    "            or((similarity(i,j)>0.45) and (similarity(df1[\"clean course description\"][df1_count],str(edx[\"What You Will Learn\"][edx_count]))>=0.0)))):\n",
    "                l=[]\n",
    "                l.append(i)\n",
    "                l.append(j)\n",
    "                l.append(df1[\"clean course description\"][df1_count])\n",
    "                l.append(edx[\"Description\"][edx_count])\n",
    "            edx_count=edx_count+1\n",
    "        l_f.append(l)\n",
    "        df1_count=df1_count+1\n",
    "\n",
    "    df2=pd.DataFrame(l_f)\n",
    "    df2.dropna(inplace=True)\n",
    "\n",
    "    #Refining and storing it in a new dataframe\n",
    "    df3[\"course title\"]=df2[0]\n",
    "    df3[\"ideal course title\"]=df2[1]\n",
    "    df3[\"course description\"]=df2[2]\n",
    "    df3[\"ideal course description\"]=df2[3]\n",
    "    df_final = pd.DataFrame.drop_duplicates(df3)\n",
    "\n",
    "    #print the dataframe \n",
    "    #print(df_final)\n",
    "    \n",
    "##############################################################################################################\n",
    "\n",
    "#function for retrieving text from html scripts\n",
    "    def data_cleaning(data,empty_list):\n",
    "        for doc in data:\n",
    "            #html=strip_tags(str(doc))\n",
    "            html=BeautifulSoup(str(doc), 'lxml').get_text()\n",
    "            dig=re.sub(\" \\d+\", \" \", html)\n",
    "            sp_char=re.sub('[^A-Za-z0-9]+', ' ', dig)\n",
    "            empty_list.append(sp_char)\n",
    "    df1=pd.DataFrame()\n",
    "    df=pd.read_csv(\"Courses.csv\")\n",
    "    df1[\"course title\"]=df[\"course_title\"]\n",
    "    df1[\"course description\"]=df[\"course_description\"]\n",
    "    df1[\"course subtitle\"]=df[\"course_subtitle\"]\n",
    "\n",
    "    #refining and storing it in dataframe d1\n",
    "    clean_cd=[]\n",
    "    data_cleaning(df1['course description'],clean_cd)\n",
    "    df1['clean course description']=clean_cd\n",
    "    df1.drop(['course description'],axis=1,inplace=True)\n",
    "\n",
    "##############################################################################################################\n",
    "\n",
    "#caller modules\n",
    "#calling spelling correction module\n",
    "i=0\n",
    "for s in df1[\"clean course description\"]:\n",
    "    \n",
    "    print(Spelling_correction(s))\n",
    "    i=i+1\n",
    "#calling grammer check module    \n",
    "i=0\n",
    "for s in df1[\"clean course description\"]:\n",
    "    \n",
    "    print(\"in the course \",df1['course title'][i],\" \",grammer_check(s))\n",
    "    i=i+1\n",
    "#calling validation module\n",
    "validation(df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original text:  This course is designed to tech. you about machine learning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' His course is designed to teach. you about machine learning'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Spelling_correction(string):\n",
    "    \n",
    "    print(\"original text: \"+str(string)) \n",
    "    b = TextBlob(string) \n",
    "    return(str(b.correct()))\n",
    "\n",
    "\n",
    "Spelling_correction(\" This course is designed to tech. you about machine learning\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(string):\n",
    "    X=str(string)\n",
    "    X = X.lower() \n",
    "\n",
    "    # tokenization \n",
    "    X_list = word_tokenize(X)  \n",
    "\n",
    "    # sw contains the list of stopwords \n",
    "    sw = stopwords.words('english')  \n",
    "    l1 =[]\n",
    "    \n",
    "    # remove stop words from string \n",
    "    X_set = {w for w in X_list if not w in sw}  \n",
    "    return(X_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"Hy boss I am not well. Can you please indentify me.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.', 'boss', 'hy', 'indentify', 'please', 'well'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hy',\n",
       " 'boss',\n",
       " 'i',\n",
       " 'am',\n",
       " 'not',\n",
       " 'well',\n",
       " '.',\n",
       " 'can',\n",
       " 'you',\n",
       " 'please',\n",
       " 'indentify',\n",
       " 'me',\n",
       " '.']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sw = stopwords.words('english')\n",
    "sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "import re\n",
    "\n",
    "\n",
    "def spellingcorrection(string):\n",
    "    spell = SpellChecker()\n",
    "    l=(re.split(';|,| |\\*|\\n',string))\n",
    "    # find those words that may be misspelled\n",
    "    misspelled = spell.unknown(l)\n",
    "\n",
    "    for word in misspelled:\n",
    "        # Get the one `most likely` answer\n",
    "        print(spell.correction(word))\n",
    "\n",
    "        # Get a list of `likely` options\n",
    "        print(spell.candidates(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thereof\n",
      "{'therein', 'thereof', 'theresia', 'theremin'}\n",
      "supposed.....\n",
      "{'supposed.....'}\n",
      "of\n",
      "{'ae', 'm9', 'de', 'ib', 'vf', 'cg', 'ny', 'ls', 'ii', 'r-', 'y2', 'dn', 'tb', 'kr', 'cv', 'q2', 'li', 'et', 'fe', 'm7', 'f3', 'xs', 'kn', 'x', 'rp', 's0', 't2', 'dk', 'f5', 'a2', 'a5', 's3', 'ij', 'w1', 'eu', 'p3', 'ap', 'hf', 'ge', 'ts', 'ey', 'rd', 'ut', 'sw', 'to', 'bu', 'kv', 'so', 'vw', 'q9', 'r8', 'h3', 'l9', 'b-', 'f7', 'uc', 'pe', 'tn', 'gn', 't4', 'g3', 'd8', 'sf', 'ra', 'vt', 'a+', 'ys', 'f', 'td', 'be', 'gu', 'ju', 'or', 'm6', 'af', 'xy', 'wa', 'nb', 'eh', 'z+', 'sa', 'bg', 'mr', 'll', 'sz', 'hc', 'q8', 'n5', 'eo', 'ar', 'r3', 'ks', 'od', 'v1', 'h2', 'bp', 'av', 'pp', 'lr', 'y1', 'c', 'nj', 'fw', \"n'\", 'w8', 'dy', 'e6', 'iv', 'eg', 'l4', 'ty', 've', 'n2', 'id', 'k.', 'hw', 'cc', 'nv', 'sr', 'xz', 'dx', 'hh', 'me', 'nz', 'bn', 'l6', 'at', 'rj', 'nc', 'b3', 'fu', 'e3', 'en', 'mi', 'z1', 'du', 'zn', 'kk', 'qp', 'w ', 'bx', 'nu', 'dj', 'ca', 'pd', 'fn', 'iz', 'rb', \"r'\", 'il', 'z3', 'ed', 'yz', 'it', 'ri', 'ec', 'gg', 'lf', 'lt', 'ic', 'ka', 'x2', 'jh', 't5', 'e', 'lb', 'ih', 'yo', 'q3', 'ly', 'hq', 'yi', 'sh', 'bc', 'oy', 'gt', 'jl', 'v6', 'ce', 'gh', 'cw', 'f6', 's7', 'ci', 'l-', 'og', 'e1', 'z2', 'a4', 'qm', 'p=', 'cb', 'ig', 'sy', 'd2', 'n6', 'd5', 'tx', 'pf', 'df', 'c2', 'm3', 'lz', 'kx', 'kg', 'px', 'by', 'bw', 'r9', 'f2', 'ia', 'es', 'xp', 'n', 'da', 'g6', 'j', 'iu', 'v.', 'x.', 'jo', 'o.', 'ac', 'au', 'a.', 'ak', 'vh', 'dr', \"s'\", 'g4', 'e2', 'p', 'jb', 'b6', 'if', 'hn', 'ba', 'u6', 'cd', 'am', 'jc', 'x-', 'r.', 'g-', 'cx', 'mc', 'se', 'n.', 'gp', 'je', 't', 'g8', 'gs', 'p.', 'm4', 'h*', 'oe', 'p6', 't6', 'd1', 'ix', 'ww', 'r7', 'fc', 'r', 'mb', 'mk', 'l7', 'mg', 'bb', 'si', 'z.', 'sg', 'vc', 'uk', 'rw', 'jp', 'aj', 'cj', 'd', 'l.', 'nk', 'ld', 'cs', 'r5', 'fb', 'y.', 'gm', 'pj', 'oc', 't.', 'tj', 'ah', 'ux', 'l', 'gy', 'tt', 'km', 'os', 'sl', 's.', 'r2', 'sj', 'fi', 'fl', 'm0', \"t'\", 'mp', 'w6', 'ad', 'c5', 'cy', 'mf', 'la', 'a', 'rl', 'g7', 'o', 'pk', 'b+', 'b', 'hr', 'ml', 'dp', 'aw', 'im', 'c7', 'ta', 'mw', 'r1', 'js', 'j.', 'le', 'sb', 'xj', 'ho', 'ok', 'ai', 'ol', 'f.', 'c3', 'b7', 'n1', 'er', 'ru', 't9', 'ng', 'gq', 'yr', 'hu', 'gb', 'u1', 'l+', 'a-', 'an', 'b5', 'rn', 'qa', 'sv', 'kc', 'q.', 'ip', 'z', 'zo', 'oo', 'cn', 'a6', 'ns', 'dz', 'lg', 'wo', 'th', 'm1', 'ry', 'nl', 'p7', 'hl', 'my', 'ck', \"a'\", 'xa', 'ur', 'c.', 'ax', 'v2', 'l5', 'w3', 'ox', 'ao', 'ee', 'ei', 'te', 'xr', 'us', 'lo', 'pa', 'mo', 'aa', 'lp', 'sk', 'k2', 'fk', 'on', 'ti', 'ha', 'jr', 'hb', 'q7', 'ws', 'vm', 'vr', 'lu', 'rt', 'eq', 'w5', 'jd', 'qc', 'lv', 'n7', 'yu', 'mq', 'h1', 'jf', 'xv', 'su', 'fx', 'mv', 'h4', 'wi', 'm-', 'w2', 'ds', 'nn', 'v8', 'tk', 'n+', 'd.', 'pq', 'xx', 'ma', 'bl', 'na', 'c+', 'gr', 'mh', 'pl', 'l2', 'bs', 'n4', 'sp', 'vo', 'kh', 'k4', 'd4', 'bt', 'ct', 'hs', 'fo', 'pn', 'rx', 'co', 'ie', 'c4', 'op', 'jj', 'wp', 'qt', 'ov', 'qb', 'lh', 'bk', 'rh', 'bh', 'i.', 'sq', 'wm', 'qv', 'r0', 's2', 's4', 'cl', 'd-', 'd7', 'nt', 'm5', 'st', 'ot', 'ja', 'ki', 't=', 's8', 'g.', 'ms', 'c9', 'mx', \"o'\", 'yd', 'b1', 'y', 's9', 'x3', 'p-', 'n=', \"m'\", 'ow', 'bo', 'dd', 'q1', \"y'\", 'pu', 'a7', 'a1', 'dv', 'g/', 'yy', 'mz', 'zp', 'lj', 'rg', 'of', 'i1', 'q4', 'qr', 'pb', 'bf', 'pt', 'ef', 'ex', 'tm', 'bv', 'v', 'ln', 'wg', 'h', 'u', 'xt', 'nm', 'g', 'g5', 'we', 'b4', 'xm', 'va', 'ne', 'gx', 'pr', \"f'\", 'kb', 'sn', 'w4', 'tl', 'wn', 'kp', 'gc', 'fg', 'jm', 'vs', 'al', 'np', 'ya', 'h7', 'uu', 'gd', 'kd', 'fm', 'y-', 'fp', 'qu', 'oh', 'dl', 'el', 'vp', 'rv', \"q'\", 'ud', 'w', 'rc', 'c-', 'rm', 'e-', 'fs', 'he', 'go', 'wd', 'fy', 'eb', 'ob', 'bq', 'a9', 'gw', 'ub', 'ea', 'ft', 'hp', 'ev', 'fr', 'br', 'wt', 'tw', 'e4', 'f+', 'lm', 'u.', 'g2', 'm8', 'y ', 'kw', 'bd', 'xc', 'vl', \"e'\", 'b8', 'sc', 'lx', 'p9', 'k3', 'ir', 'c6', 'q', 'd/', 'hg', 's', 'h6', 'd9', 'zu', 'ek', 'ep', 'vd', 'n-', 'vg', 'w7', 'dt', 'ji', 'gv', 'ph', 'yt', 'r4', 'hj', 'x4', 'a8', 'zz', 'c1', 'l3', 'nh', 'do', 'um', 'no', 'lw', 'g9', 'f8', 'vj', 'zx', 'p2', 'ye', 'q6', 's6', 'wh', 'ni', 'xd', 'ch', 'e8', \"c'\", 'f1', 'sm', 'i/', 'cf', 'pw', 'hk', 'ab', 'x1', 'b.', 'wb', 'u2', 'vu', 'qs', 'po', 'xu', 'in', 'ag', 'k', 'i-', 's1', 'ew', 'ro', 'jt', 'c8', 'c/', 'ky', 'bi', 'l1', 'un', 'ua', 'h.', 'r=', 'sx', 'v4', 'gi', 'p5', 'pc', 'ga', 'vv', 't-', 'm.', 'cp', 'qi', 'p1', 'kf', \"p'\", 'b9', 'ui', 'ko', \"d'\", 't3', 'w.', 'a*', 'ke', 'ff', 'p%', 'dw', 'i', 'tg', 'ze', 'e7', 'io', 'l*', 'p4', 'oa', 'tu', 'bm', 'iq', 'em', 'dm', 'pg', 'mt', 'fa', 'b2', 'v9', 'is', 'x%', 'ay', 'uv', 'o2', 'tr', 'uh', 'sd', 'rr', 'oi', 'kt', \"j'\", 'qd', 'mn', 'ku', 'p8', 'jn', 'bj', 'f4', 'cu', 'pi', 'm', 'ps', 'f9', 't7', 'm2', 'd6', 'ou', 'nw', 'wc', 'o%', 'dg', 'md', 'h8', 'hm', 'uf', 'cr', 'r6', 'mm', 'cm', 'nf', 'd3', 'gl', 'wr', 'pm', 'r*', 'rf', 'qw', 'dc', 's5', 'dh', 'a3', 't1', 'fv', 'rk', 'tv', 'hi', 'wu', 'n3', 'e.', 'g1', 'oz', 'yn', 'w9', 'kj', 'db', 'xi', 'yc', 'mu', 'tp', 'vb', 'vi', \"i'\", 'hd', 're', 'k1', 'r/', 'lc', 'ht', 'h5', 'as', 'up', 'l8', 'mj', 'tc', 'xl', 'e5', 'z%', 'nr', 'om', 'dq', 'nd', 'rs', 'ss', 'hz', 'kl', 'pv', 'di'}\n",
      "story\n",
      "{'story'}\n",
      "done\n",
      "{'donee', 'done'}\n",
      "person\n",
      "{'persona', 'persons', 'person'}\n",
      "you\n",
      "{'you', 'your', 'yous', \"you'\"}\n",
      "you\n",
      "{'yiu', 'you', 'iou'}\n"
     ]
    }
   ],
   "source": [
    "spellingcorrection(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "take\n",
      "{'tyke', 'take', 'tek', 'tepe', 'eke', 'toke', 'tete', 'keke', 'tele', 'teme', 'zeke', 'tee', 'tuke', 'teske'}\n",
      "makes\n",
      "{'muses', 'lukes', 'mikes', 'mules', 'mutes', 'muker', 'mures', 'makes', 'dukes', 'jukes'}\n",
      "project\n",
      "{'project'}\n"
     ]
    }
   ],
   "source": [
    "spellingcorrection(\"Mr oye mukes good priject So wi can teke him\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grammer_check(string):\n",
    "    \n",
    "\n",
    "    from pylanguagetool import api\n",
    "   \n",
    "    if(api.check(string,api_url='https://languagetool.org/api/v2/',lang='en-US', )['matches']==[]):\n",
    "        return(\"no gramatical error.\")\n",
    "    else:\n",
    "        s=api.check(string,api_url='https://languagetool.org/api/v2/',lang='en-US', )['matches'][0]['message']\n",
    "        return(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'no gramatical error.'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grammer_check(\"This ..... is an apple...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Use past participle here: \"gone\".'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grammer_check(\"I have go there\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did you mean \"I\"?\n"
     ]
    }
   ],
   "source": [
    "string=\"I am not okay but I would be okay whenever i will can do this\"\n",
    "from pylanguagetool import api\n",
    "   \n",
    "if(api.check(string,api_url='https://languagetool.org/api/v2/',lang='en-US', )['matches']==[]):\n",
    "    print(\"no gramatical error.\")\n",
    "else:\n",
    "    s=api.check(string,api_url='https://languagetool.org/api/v2/',lang='en-US', )['matches'][0]['message']\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'message': 'Did you mean \"I\"?', 'shortMessage': 'Possible typo', 'replacements': [{'value': 'I'}], 'offset': 43, 'length': 1, 'context': {'text': '...m not okay but I would be okay whenever i will can do this', 'offset': 43, 'length': 1}, 'sentence': 'I am not okay but I would be okay whenever i will can do this', 'type': {'typeName': 'Other'}, 'rule': {'id': 'I_LOWERCASE', 'subId': '3', 'description': 'i vs. I', 'issueType': 'misspelling', 'category': {'id': 'TYPOS', 'name': 'Possible Typo'}}, 'ignoreForIncompleteSentence': True, 'contextForSureMatch': 5}, {'message': 'Two modal verbs connected. Did you mean: \"will, can\", \"will\" or \"can\"?', 'shortMessage': '', 'replacements': [{'value': 'will, can'}, {'value': 'will'}, {'value': 'can'}], 'offset': 45, 'length': 8, 'context': {'text': '...not okay but I would be okay whenever i will can do this', 'offset': 43, 'length': 8}, 'sentence': 'I am not okay but I would be okay whenever i will can do this', 'type': {'typeName': 'Other'}, 'rule': {'id': 'TWO_CONNECTED_MODAL_VERBS', 'subId': '1', 'description': 'Two modal verbs in a row (could should)', 'issueType': 'grammar', 'category': {'id': 'GRAMMAR', 'name': 'Grammar'}}, 'ignoreForIncompleteSentence': False, 'contextForSureMatch': 3}]\n"
     ]
    }
   ],
   "source": [
    "s=api.check(string,api_url='https://languagetool.org/api/v2/',lang='en-US', )['matches']\n",
    "#[0]['message']\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did you mean \"I\"?\n",
      "Two modal verbs connected. Did you mean: \"will, can\", \"will\" or \"can\"?\n"
     ]
    }
   ],
   "source": [
    "s=api.check(string,api_url='https://languagetool.org/api/v2/',lang='en-US', )['matches']\n",
    "for i in s:\n",
    "    print(i[\"message\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "string=\"How !! am i supposed..... to know this when you are not there.If you were there then i could have shared with you. you dont know how much i love you. if you ever get a chance please come to me i will tell yiou the whole story. dont do the same mistake that i have done. so that you can be a good person.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_string=\"\"\n",
    "i=0\n",
    "for s in string:\n",
    "    if(s==\".\" and i!=(len(string)-1) and string[i+1]==\".\"):\n",
    "        tttt=0\n",
    "    elif(s==\".\" and i!=0 and string[i-1]!=\".\"):\n",
    "        new_string=new_string+ \" . \"\n",
    "    elif( ord(s)>=97 and ord(s)<=122 or ord(s)>=65 and ord(s)<=90 or s==\" \"):\n",
    "        new_string=new_string+s\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How  am i supposed to know this when you are not there . If you were there then i could have shared with you .  you dont know how much i love you .  if you ever get a chance please come to me i will tell yiou the whole story .  dont do the same mistake that i have done .  so that you can be a good person . '"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "string2=\"\"\n",
    "for s in new_string:\n",
    "    if(s==\".\" and new_string[i+1]==\".\"):\n",
    "        t=0\n",
    "    else:\n",
    "        string2=string2+s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'string' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-edbf08a562d5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'string' is not defined"
     ]
    }
   ],
   "source": [
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 're' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-148fdbb515c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ml\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\" \"|\\*|\\n'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 're' is not defined"
     ]
    }
   ],
   "source": [
    "l=(re.split('\" \"|\\*|\\n',string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How !! am i supposed..... to know this when you are not there.If you were there then i could have shared with you. you dont know how much i love you. if you ever get a chance please come to me i will tell yiou the whole story. dont do the same mistake that i have done. so that you can be a good person.']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=(string.split('.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How !! am i supposed',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ' to know this when you are not there',\n",
       " 'If you were there then i could have shared with you',\n",
       " ' you dont know how much i love you',\n",
       " ' if you ever get a chance please come to me i will tell yiou the whole story',\n",
       " ' dont do the same mistake that i have done',\n",
       " ' so that you can be a good person',\n",
       " '']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grammer_check(string):\n",
    "    \n",
    "\n",
    "    from pylanguagetool import api\n",
    "   \n",
    "    if(api.check(string,api_url='https://languagetool.org/api/v2/',lang='en-US', )['matches']==[]):\n",
    "        return(\"no gramatical error.\")\n",
    "    else:\n",
    "        s=api.check(string,api_url='https://languagetool.org/api/v2/',lang='en-US', )['matches'][0]['message']\n",
    "        return(s)\n",
    "\n",
    "\n",
    "\n",
    "def grammer_check(string):\n",
    "    count=0\n",
    "    temp_string=\"\"\n",
    "    l=len(string)\n",
    "    output=[]\n",
    "    s=string\n",
    "    from pylanguagetool import api\n",
    "    for i in range(l-1):\n",
    "        if(s[i]==\".\" and s[i+1]==\".\"):\n",
    "\n",
    "            output.append(\"Repeatation of punctuation in line \"+str(count))\n",
    "        elif(s[i]==\".\"):\n",
    "            #print(temp_string)\n",
    "            #print(\"\\n\")\n",
    "            result = grammer_check(temp_string)\n",
    "            check=api.check(temp_string,api_url='https://languagetool.org/api/v2/',lang='en-US', )['matches']\n",
    "            for i in check:\n",
    "                output.append(i[\"message\"] + \" in line \"+str(count))\n",
    "            count=count+1\n",
    "            temp_string=\"\"\n",
    "        elif(s[i]==s[i+1] and ((ord(s[i])<65 and ord(s[i])>90 )or(ord(s[i])<97 and ord(s[i])>122))):\n",
    "\n",
    "            output.append(\"Repeatition of punctuation in line \"+str(count))\n",
    "        else:\n",
    "            temp_string=temp_string+s[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grammer_check(string):\n",
    "    count=0\n",
    "    temp_string=\"\"\n",
    "    l=len(string)\n",
    "    output=[]\n",
    "    s=string\n",
    "    from pylanguagetool import api\n",
    "    for i in range(l-1):\n",
    "        if(s[i]==\".\" and s[i+1]==\".\"):\n",
    "\n",
    "            output.append(\"Repeatition of punctuation in line \"+str(count))\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        elif(s[i]==\".\"):\n",
    "            \n",
    "            result = grammer_check(temp_string)\n",
    "            check=api.check(temp_string,api_url='https://languagetool.org/api/v2/',lang='en-US', )['matches']\n",
    "            for i in check:\n",
    "                output.append(i[\"message\"] + \" in line \"+str(count))\n",
    "            count=count+1\n",
    "            temp_string=\"\"\n",
    "            \n",
    "            \n",
    "            \n",
    "        elif(s[i]==s[i+1] and ((ord(s[i])<65 and ord(s[i])>90 )or(ord(s[i])<97 and ord(s[i])>122))):\n",
    "\n",
    "            output.append(\"Repeatition of punctuation in line \"+str(count))\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        else:\n",
    "            temp_string=temp_string+s[i]\n",
    "    for i in range(len(output)-1):\n",
    "        if(output[i]!=output[i+1]):\n",
    "            print(output[i])\n",
    "    #print(output[-1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How !! am i supposed..... to know this when you are not there.If you were there then i could have shared with you. you dont know how much i love you. if you ever get a chance please come to me i will tell yiou the whole story. dont do the same mistake that i have done. so that you can be a good person.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeatation of punctuation in line 0\n",
      "This sentence does not start with an uppercase letter in line 0\n",
      "Is this the personal pronoun \"I\"? It is spelled uppercase. in line 0\n",
      "Please add a punctuation mark at the end of paragraph in line 0\n",
      "This sentence does not start with an uppercase letter in line 1\n",
      "Did you mean \"I\"? in line 2\n",
      "This sentence does not start with an uppercase letter in line 3\n",
      "Possible spelling mistake found in line 3\n",
      "Is this the personal pronoun \"I\"? It is spelled uppercase. in line 3\n",
      "This sentence does not start with an uppercase letter in line 4\n",
      "Did you mean \"I\"? in line 4\n",
      "Possible spelling mistake found. in line 4\n",
      "Possible spelling mistake found in line 5\n"
     ]
    }
   ],
   "source": [
    "grammer_check(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "import re\n",
    "\n",
    "\n",
    "def spellingcorrection(string):\n",
    "    spell = SpellChecker()\n",
    "    l=(re.split(';|,| |\\*|\\n',string))\n",
    "    # find those words that may be misspelled\n",
    "    misspelled = spell.unknown(l)\n",
    "\n",
    "    for word in misspelled:\n",
    "        # Get the one `most likely` answer\n",
    "        print(spell.correction(word))\n",
    "\n",
    "        # Get a list of `likely` options\n",
    "        print(spell.candidates(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thereof\n",
      "{'therein', 'thereof', 'theresia', 'theremin'}\n",
      "supposed.....\n",
      "{'supposed.....'}\n",
      "of\n",
      "{'ae', 'm9', 'de', 'ib', 'vf', 'cg', 'ny', 'ls', 'ii', 'r-', 'y2', 'dn', 'tb', 'kr', 'cv', 'q2', 'li', 'et', 'fe', 'm7', 'f3', 'xs', 'kn', 'x', 'rp', 's0', 't2', 'dk', 'f5', 'a2', 'a5', 's3', 'ij', 'w1', 'eu', 'p3', 'ap', 'hf', 'ge', 'ts', 'ey', 'rd', 'ut', 'sw', 'to', 'bu', 'kv', 'so', 'vw', 'q9', 'r8', 'h3', 'l9', 'b-', 'f7', 'uc', 'pe', 'tn', 'gn', 't4', 'g3', 'd8', 'sf', 'ra', 'vt', 'a+', 'ys', 'f', 'td', 'be', 'gu', 'ju', 'or', 'm6', 'af', 'xy', 'wa', 'nb', 'eh', 'z+', 'sa', 'bg', 'mr', 'll', 'sz', 'hc', 'q8', 'n5', 'eo', 'ar', 'r3', 'ks', 'od', 'v1', 'h2', 'bp', 'av', 'pp', 'lr', 'y1', 'c', 'nj', 'fw', \"n'\", 'w8', 'dy', 'e6', 'iv', 'eg', 'l4', 'ty', 've', 'n2', 'id', 'k.', 'hw', 'cc', 'nv', 'sr', 'xz', 'dx', 'hh', 'me', 'nz', 'bn', 'l6', 'at', 'rj', 'nc', 'b3', 'fu', 'e3', 'en', 'mi', 'z1', 'du', 'zn', 'kk', 'qp', 'w ', 'bx', 'nu', 'dj', 'ca', 'pd', 'fn', 'iz', 'rb', \"r'\", 'il', 'z3', 'ed', 'yz', 'it', 'ri', 'ec', 'gg', 'lf', 'lt', 'ic', 'ka', 'x2', 'jh', 't5', 'e', 'lb', 'ih', 'yo', 'q3', 'ly', 'hq', 'yi', 'sh', 'bc', 'oy', 'gt', 'jl', 'v6', 'ce', 'gh', 'cw', 'f6', 's7', 'ci', 'l-', 'og', 'e1', 'z2', 'a4', 'qm', 'p=', 'cb', 'ig', 'sy', 'd2', 'n6', 'd5', 'tx', 'pf', 'df', 'c2', 'm3', 'lz', 'kx', 'kg', 'px', 'by', 'bw', 'r9', 'f2', 'ia', 'es', 'xp', 'n', 'da', 'g6', 'j', 'iu', 'v.', 'x.', 'jo', 'o.', 'ac', 'au', 'a.', 'ak', 'vh', 'dr', \"s'\", 'g4', 'e2', 'p', 'jb', 'b6', 'if', 'hn', 'ba', 'u6', 'cd', 'am', 'jc', 'x-', 'r.', 'g-', 'cx', 'mc', 'se', 'n.', 'gp', 'je', 't', 'g8', 'gs', 'p.', 'm4', 'h*', 'oe', 'p6', 't6', 'd1', 'ix', 'ww', 'r7', 'fc', 'r', 'mb', 'mk', 'l7', 'mg', 'bb', 'si', 'z.', 'sg', 'vc', 'uk', 'rw', 'jp', 'aj', 'cj', 'd', 'l.', 'nk', 'ld', 'cs', 'r5', 'fb', 'y.', 'gm', 'pj', 'oc', 't.', 'tj', 'ah', 'ux', 'l', 'gy', 'tt', 'km', 'os', 'sl', 's.', 'r2', 'sj', 'fi', 'fl', 'm0', \"t'\", 'mp', 'w6', 'ad', 'c5', 'cy', 'mf', 'la', 'a', 'rl', 'g7', 'o', 'pk', 'b+', 'b', 'hr', 'ml', 'dp', 'aw', 'im', 'c7', 'ta', 'mw', 'r1', 'js', 'j.', 'le', 'sb', 'xj', 'ho', 'ok', 'ai', 'ol', 'f.', 'c3', 'b7', 'n1', 'er', 'ru', 't9', 'ng', 'gq', 'yr', 'hu', 'gb', 'u1', 'l+', 'a-', 'an', 'b5', 'rn', 'qa', 'sv', 'kc', 'q.', 'ip', 'z', 'zo', 'oo', 'cn', 'a6', 'ns', 'dz', 'lg', 'wo', 'th', 'm1', 'ry', 'nl', 'p7', 'hl', 'my', 'ck', \"a'\", 'xa', 'ur', 'c.', 'ax', 'v2', 'l5', 'w3', 'ox', 'ao', 'ee', 'ei', 'te', 'xr', 'us', 'lo', 'pa', 'mo', 'aa', 'lp', 'sk', 'k2', 'fk', 'on', 'ti', 'ha', 'jr', 'hb', 'q7', 'ws', 'vm', 'vr', 'lu', 'rt', 'eq', 'w5', 'jd', 'qc', 'lv', 'n7', 'yu', 'mq', 'h1', 'jf', 'xv', 'su', 'fx', 'mv', 'h4', 'wi', 'm-', 'w2', 'ds', 'nn', 'v8', 'tk', 'n+', 'd.', 'pq', 'xx', 'ma', 'bl', 'na', 'c+', 'gr', 'mh', 'pl', 'l2', 'bs', 'n4', 'sp', 'vo', 'kh', 'k4', 'd4', 'bt', 'ct', 'hs', 'fo', 'pn', 'rx', 'co', 'ie', 'c4', 'op', 'jj', 'wp', 'qt', 'ov', 'qb', 'lh', 'bk', 'rh', 'bh', 'i.', 'sq', 'wm', 'qv', 'r0', 's2', 's4', 'cl', 'd-', 'd7', 'nt', 'm5', 'st', 'ot', 'ja', 'ki', 't=', 's8', 'g.', 'ms', 'c9', 'mx', \"o'\", 'yd', 'b1', 'y', 's9', 'x3', 'p-', 'n=', \"m'\", 'ow', 'bo', 'dd', 'q1', \"y'\", 'pu', 'a7', 'a1', 'dv', 'g/', 'yy', 'mz', 'zp', 'lj', 'rg', 'of', 'i1', 'q4', 'qr', 'pb', 'bf', 'pt', 'ef', 'ex', 'tm', 'bv', 'v', 'ln', 'wg', 'h', 'u', 'xt', 'nm', 'g', 'g5', 'we', 'b4', 'xm', 'va', 'ne', 'gx', 'pr', \"f'\", 'kb', 'sn', 'w4', 'tl', 'wn', 'kp', 'gc', 'fg', 'jm', 'vs', 'al', 'np', 'ya', 'h7', 'uu', 'gd', 'kd', 'fm', 'y-', 'fp', 'qu', 'oh', 'dl', 'el', 'vp', 'rv', \"q'\", 'ud', 'w', 'rc', 'c-', 'rm', 'e-', 'fs', 'he', 'go', 'wd', 'fy', 'eb', 'ob', 'bq', 'a9', 'gw', 'ub', 'ea', 'ft', 'hp', 'ev', 'fr', 'br', 'wt', 'tw', 'e4', 'f+', 'lm', 'u.', 'g2', 'm8', 'y ', 'kw', 'bd', 'xc', 'vl', \"e'\", 'b8', 'sc', 'lx', 'p9', 'k3', 'ir', 'c6', 'q', 'd/', 'hg', 's', 'h6', 'd9', 'zu', 'ek', 'ep', 'vd', 'n-', 'vg', 'w7', 'dt', 'ji', 'gv', 'ph', 'yt', 'r4', 'hj', 'x4', 'a8', 'zz', 'c1', 'l3', 'nh', 'do', 'um', 'no', 'lw', 'g9', 'f8', 'vj', 'zx', 'p2', 'ye', 'q6', 's6', 'wh', 'ni', 'xd', 'ch', 'e8', \"c'\", 'f1', 'sm', 'i/', 'cf', 'pw', 'hk', 'ab', 'x1', 'b.', 'wb', 'u2', 'vu', 'qs', 'po', 'xu', 'in', 'ag', 'k', 'i-', 's1', 'ew', 'ro', 'jt', 'c8', 'c/', 'ky', 'bi', 'l1', 'un', 'ua', 'h.', 'r=', 'sx', 'v4', 'gi', 'p5', 'pc', 'ga', 'vv', 't-', 'm.', 'cp', 'qi', 'p1', 'kf', \"p'\", 'b9', 'ui', 'ko', \"d'\", 't3', 'w.', 'a*', 'ke', 'ff', 'p%', 'dw', 'i', 'tg', 'ze', 'e7', 'io', 'l*', 'p4', 'oa', 'tu', 'bm', 'iq', 'em', 'dm', 'pg', 'mt', 'fa', 'b2', 'v9', 'is', 'x%', 'ay', 'uv', 'o2', 'tr', 'uh', 'sd', 'rr', 'oi', 'kt', \"j'\", 'qd', 'mn', 'ku', 'p8', 'jn', 'bj', 'f4', 'cu', 'pi', 'm', 'ps', 'f9', 't7', 'm2', 'd6', 'ou', 'nw', 'wc', 'o%', 'dg', 'md', 'h8', 'hm', 'uf', 'cr', 'r6', 'mm', 'cm', 'nf', 'd3', 'gl', 'wr', 'pm', 'r*', 'rf', 'qw', 'dc', 's5', 'dh', 'a3', 't1', 'fv', 'rk', 'tv', 'hi', 'wu', 'n3', 'e.', 'g1', 'oz', 'yn', 'w9', 'kj', 'db', 'xi', 'yc', 'mu', 'tp', 'vb', 'vi', \"i'\", 'hd', 're', 'k1', 'r/', 'lc', 'ht', 'h5', 'as', 'up', 'l8', 'mj', 'tc', 'xl', 'e5', 'z%', 'nr', 'om', 'dq', 'nd', 'rs', 'ss', 'hz', 'kl', 'pv', 'di'}\n",
      "story\n",
      "{'story'}\n",
      "done\n",
      "{'donee', 'done'}\n",
      "person\n",
      "{'persona', 'persons', 'person'}\n",
      "you\n",
      "{'you', 'your', 'yous', \"you'\"}\n",
      "you\n",
      "{'yiu', 'you', 'iou'}\n"
     ]
    }
   ],
   "source": [
    "spellingcorrection(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How !! am i supposed..... to know this when you are not there.If you were there then i could have shared with you. you dont know how much i love you. if you ever get a chance please come to me i will tell yiou the whole story. dont do the same mistake that i have done. so that you can be a good person.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "import re\n",
    "def spell_check(string):\n",
    "    count=0\n",
    "    output=[]\n",
    "    l=string.split(\".\")\n",
    "    length=len(l)\n",
    "    spell = SpellChecker()\n",
    "    for i in range(length-1):\n",
    "        if(l[i+1]!=\"\"):\n",
    "            j=(re.split(';|,| |\\*|\\n',l[i]))\n",
    "            # find those words that may be misspelled\n",
    "            misspelled = spell.unknown(j)\n",
    "\n",
    "            for word in misspelled:\n",
    "                \n",
    "                if(spell.correction(word)!=word):\n",
    "                    output.append(\"A spelling error in line \"+l[i])\n",
    "                    output.append(spell.correction(word) + \" is the most likely word \")\n",
    "            count=count+1\n",
    "    for i in range(len(output)-1):\n",
    "        if(output[i]!=output[i+1]):\n",
    "            print(output[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A spelling error in line \n",
      "a is the most likely word \n",
      "A spelling error in line  to know this when you are not there\n",
      "a is the most likely word \n",
      "A spelling error in line  you dont know how much i love you\n",
      "a is the most likely word \n",
      "A spelling error in line  if you ever get a chance please come to me i will tell yiou the whole story\n",
      "a is the most likely word \n",
      "A spelling error in line  if you ever get a chance please come to me i will tell yiou the whole story\n",
      "you is the most likely word \n",
      "A spelling error in line  dont do the same mistake that i have done\n"
     ]
    }
   ],
   "source": [
    "spell_check(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spelling_check(string):\n",
    "    count=0\n",
    "    temp_string=\"\"\n",
    "    l=len(string)\n",
    "    output=[]\n",
    "    s=string\n",
    "    \n",
    "    for i in range(l-1):\n",
    "        if(s[i]==\".\" and s[i+1]==\".\"):\n",
    "            nnnn=0\n",
    "        elif(s[i]==\".\"):\n",
    "            \n",
    "            result = grammer_check(temp_string)\n",
    "            check=api.check(temp_string,api_url='https://languagetool.org/api/v2/',lang='en-US', )['matches']\n",
    "            for i in check:\n",
    "                output.append(i[\"message\"] + \" in line \"+str(count))\n",
    "                \n",
    "            ###############################################################################\n",
    "            spell = SpellChecker()\n",
    "            l=(re.split(';|,| |\\*|\\n',temp_string))\n",
    "            # find those words that may be misspelled\n",
    "            misspelled = spell.unknown(l)\n",
    "\n",
    "            for word in misspelled:\n",
    "                \n",
    "                if(spell.correction(word)!=word):\n",
    "                    output.append(\"A spelling error in line \"+str(count))\n",
    "                    output.append(spell.correction(word) + \" is the most likely word \")\n",
    "                \n",
    "                          \n",
    "            #################################################################################\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "            count=count+1\n",
    "            temp_string=\"\"\n",
    "            \n",
    "            \n",
    "            \n",
    "        elif(s[i]==s[i+1] and ((ord(s[i])<65 and ord(s[i])>90 )or(ord(s[i])<97 and ord(s[i])>122))):\n",
    "\n",
    "            output.append(\"Repeatition of punctuation in line \"+str(count))\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        else:\n",
    "            temp_string=temp_string+s[i]\n",
    "    for i in range(len(output)-1):\n",
    "        if(output[i]!=output[i+1]):\n",
    "            print(output[i])\n",
    "    #print(output[-1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How  am i supposed\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " to know this when you are not there\n",
      "If you were there then i could have shared with you\n",
      " you dont know how much i love you\n",
      " if you ever get a chance please come to me i will tell yiou the whole story\n",
      " dont do the same mistake that i have done\n",
      " so that you can be a good person\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in arr:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possible typo: you repeated a whitespace\n",
      "Is this the personal pronoun \"I\"? It is spelled uppercase.\n",
      "Add a space between sentences\n",
      "Did you mean \"I\"?\n",
      "This sentence does not start with an uppercase letter\n",
      "Possible spelling mistake found\n",
      "Is this the personal pronoun \"I\"? It is spelled uppercase.\n",
      "This sentence does not start with an uppercase letter\n",
      "Did you mean \"I\"?\n",
      "Possible spelling mistake found.\n",
      "Possible spelling mistake found\n",
      "Is this the personal pronoun \"I\"? It is spelled uppercase.\n",
      "This sentence does not start with an uppercase letter\n"
     ]
    }
   ],
   "source": [
    "s=api.check(string2,api_url='https://languagetool.org/api/v2/',lang='en-US', )['matches']\n",
    "for i in s:\n",
    "    print(i[\"message\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar_check.correct(text, matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_check(string):\n",
    "    !pip install pyspellchecker\n",
    "    from spellchecker import SpellChecker\n",
    "    \n",
    "    s1=\"\"\n",
    "    corrected_string=\"\"\n",
    "    for s in string:\n",
    "        if(s!=\" \" and s!=\".\" and s!=\";\" and s!=\",\"):\n",
    "            s1=s1+s\n",
    "\n",
    "\n",
    "        else:\n",
    "\n",
    "            \n",
    "            mp = spell.unknown([(s1)])\n",
    "            \n",
    "            if(list(mp)==[]):\n",
    "               \n",
    "                if(s==\" \"):\n",
    "                    corrected_string=corrected_string+s1+\" \"\n",
    "                else:\n",
    "                    corrected_string=corrected_string+s1+s\n",
    "                \n",
    "            else:\n",
    "                if(s==\" \"):\n",
    "                    corrected_string=corrected_string+ spell.correction(list(mp)[0])+\" \"\n",
    "                else:\n",
    "                    corrected_string=corrected_string+ spell.correction(list(mp)[0])+s\n",
    "                \n",
    "            s1=\"\"\n",
    "\n",
    "    return (corrected_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspellchecker in c:\\users\\kiit\\anaconda3\\lib\\site-packages (0.5.4)\n",
      "Requirement already satisfied: pylanguagetool in c:\\users\\kiit\\anaconda3\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: colorama>=0.4.1 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from pylanguagetool) (0.4.1)\n",
      "Requirement already satisfied: requests>=2.22.0 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from pylanguagetool) (2.22.0)\n",
      "Requirement already satisfied: configargparse>=0.14.0 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from pylanguagetool) (1.2)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from requests>=2.22.0->pylanguagetool) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from requests>=2.22.0->pylanguagetool) (1.24.2)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from requests>=2.22.0->pylanguagetool) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from requests>=2.22.0->pylanguagetool) (2019.11.28)\n",
      "Requirement already satisfied: pymysql in c:\\users\\kiit\\anaconda3\\lib\\site-packages (0.9.3)\n",
      "Requirement already satisfied: gensim in c:\\users\\kiit\\anaconda3\\lib\\site-packages (3.8.2)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from gensim) (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from gensim) (1.18.1)\n",
      "Requirement already satisfied: scipy>=1.0.0 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from gensim) (1.2.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from gensim) (1.11.1)\n",
      "Requirement already satisfied: requests in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from smart-open>=1.8.1->gensim) (2.22.0)\n",
      "Requirement already satisfied: boto3 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.12.39)\n",
      "Requirement already satisfied: boto in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from smart-open>=1.8.1->gensim) (2.49.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (1.24.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2019.11.28)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from boto3->smart-open>=1.8.1->gensim) (0.9.5)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from boto3->smart-open>=1.8.1->gensim) (0.3.3)\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.39 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from boto3->smart-open>=1.8.1->gensim) (1.15.39)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from botocore<1.16.0,>=1.15.39->boto3->smart-open>=1.8.1->gensim) (0.14)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from botocore<1.16.0,>=1.15.39->boto3->smart-open>=1.8.1->gensim) (2.7.5)\n",
      "Requirement already satisfied: textblob in c:\\users\\kiit\\anaconda3\\lib\\site-packages (0.15.3)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from textblob) (3.4.4)\n",
      "Requirement already satisfied: six in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (1.12.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\kiit\\anaconda3\\lib\\site-packages (3.4.4)\n",
      "Requirement already satisfied: six in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from nltk) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspellchecker\n",
    "!pip install pylanguagetool\n",
    "!pip install pymysql\n",
    "!pip install gensim\n",
    "!pip install textblob\n",
    "!pip install nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "import pymysql\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from gensim.parsing.preprocessing import remove_stopwords,stem_text,strip_tags\n",
    "from spellchecker import SpellChecker\n",
    "import re\n",
    "from textblob import TextBlob \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
